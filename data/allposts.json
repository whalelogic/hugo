[
  {
    "id": "1",
    "slug": "flask-rest-api-setup",
    "title": "Flask REST API Setup",
    "subtitle": "A Step-by-Step Tutorial",
    "author": "Keith Thomson",
    "content": "# Flask REST API Setup: A Step-by-Step Tutorial\n\n---\n\n## \ud83d\udccb Introduction\n\nIn this tutorial, we'll walk through setting up a **basic Flask REST API**. We'll cover the essential steps, from installing dependencies to creating routes for **CRUD operations**.\n\n---\n\n## \ud83d\udee0\ufe0f Step 1: Install Dependencies\n\nTo start, you'll need to install **Flask** and **Flask-RESTful**. Use `pip` to install them:\n\n```bash\npip install flask flask-restful\n```\n\n### \ud83d\udcc2 Step 2: Create a New Flask App\nCreate a new file called app.py and add the following code:\n```python\nfrom flask import Flask\nfrom flask_restful import Api\n\napp = Flask(__name__)\napi = Api(app)\n\n@app.route('/')\ndef home():\n    return \"Welcome to my API!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nThis sets up a basic Flask app with a single route.\n\n### \ud83d\udd27 Step 3: Define Your API Endpoints\nLet's create a simple API for managing books. We'll define endpoints for CRUD operations:\nfrom flask_restful import Resource, reqparse\n\n# Sample in-memory data store\n```python\nbooks = [\n    {\"id\": 1, \"title\": \"Book 1\", \"author\": \"Author 1\"},\n    {\"id\": 2, \"title\": \"Book 2\", \"author\": \"Author 2\"}\n]\n\nclass BookList(Resource):\n    def get(self):\n        return books\n\n    def post(self):\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str, required=True)\n        parser.add_argument(\"author\", type=str, required=True)\n        args = parser.parse_args()\n        new_book = {\n            \"id\": len(books) + 1,\n            \"title\": args[\"title\"],\n            \"author\": args[\"author\"]\n        }\n        books.append(new_book)\n        return new_book, 201\n\nclass Book(Resource):\n    def get(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        return book\n\n    def put(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        parser = reqparse.RequestParser()\n        parser.add_argument(\"title\", type=str)\n        parser.add_argument(\"author\", type=str)\n        args = parser.parse_args()\n        book[\"title\"] = args.get(\"title\", book[\"title\"])\n        book[\"author\"] = args.get(\"author\", book[\"author\"])\n        return book\n\n    def delete(self, book_id):\n        book = next((book for book in books if book[\"id\"] == book_id), None)\n        if book is None:\n            return {\"error\": \"Book not found\"}, 404\n        books.remove(book)\n        return {\"message\": \"Book deleted\"}\n\napi.add_resource(BookList, \"/books\")\napi.add_resource(Book, \"/books/<int\\:book_id>\")\n```\n\n## Endpoint Summary\n- BookListGET/booksRetrieve all books\n- BookListPOST/booksCreate a new book\n- BookGET/books/<book_id>Retrieve a single book\n- BookPUT/books/<book_id>Update a book\n- BookDELETE/books/<book_id>Delete a book\n\n### \u25b6\ufe0f Step 4: Run Your API\nRun your API using:\npython app.py\nYou can now interact with your API using tools like curl or a REST client.\n\n### \ud83d\udccc Example Use Cases\nGet all bookscurl http://localhost:5000/books \nCreate a new book \n```bash \ncurl -X POST -H \"Content-Type: application/json\" -d '{\"title\": \"New Book\", \"author\": \"New Author\"}' http://localhost:5000/booksGet a single bookcurl http://localhost:5000/books/1Update a bookcurl -X PUT -H \"Content-Type: application/json\" -d '{\"title\": \"Updated Book\"}' http://localhost:5000/books/1Delete a bookcurl -X DELETE http://localhost:5000/books/1\n```\n\n## \ud83c\udfaf Conclusion\nThis tutorial provides a basic setup for a Flask REST API. You can build upon this example to create more complex APIs with additional features like:\n",
    "summary": "A forward-looking piece on the ethical challenges of advanced AI and biotechnology in the year 2050.",
    "read_time": "8 min read",
    "tags": "Flask, REST API",
    "category": "Programming",
    "created_on": "2025-04-12 13:12:06",
    "updated_on": "2025-07-04 12:20:25",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "12",
    "slug": "python-regex",
    "title": "Regular Expressions for Python",
    "subtitle": "Harnessing the Power of Regex",
    "author": "Keith Thomson",
    "content": "# \ud83d\udd0d Mastering Regular Expressions: A Comprehensive Guide\n\n## \ud83d\udccc Introduction\n\nRegular expressions (regex) are a **\ud83d\udd25 powerful tool** for **pattern matching** and **text manipulation**. They allow you to **\ud83d\udd0d search, \ud83d\udcdd extract, and \ud83d\udd04 replace** specific patterns within strings, making them invaluable for tasks like:\n- **\u2705 Data validation**\n- **\ud83d\udcca Parsing**\n- **\ud83d\udd0e Text mining**\n- **\ud83d\udcc2 Log analysis**\n- **\ud83d\udd04 Search-and-replace operations**\n\nThis guide will introduce you to the **fundamental concepts, syntax, and real-world applications** of regular expressions.\n\n---\n\n## \ud83d\udccb Table of Contents\n1. [Basic Syntax](#basic-syntax)\n2. [Special Characters](#special-characters)\n3. [Grouping and Capturing](#grouping-and-capturing)\n4. [Lookaheads and Lookbehinds](#lookaheads-and-lookbehinds)\n5. [Common Use Cases](#common-use-cases)\n6. [Regex in Python](#regex-in-python)\n7. [Performance Considerations](#performance-considerations)\n8. [Practical Examples](#practical-examples)\n9. [Debugging and Testing](#debugging-and-testing)\n10. [Conclusion](#conclusion)\n\n---\n\n## \ud83d\udcd6 Basic Syntax \n\n### \ud83d\udd24 Literal Characters\nMatch **exact characters**. For example, the regex `hello` will match the string `\"hello\"`.\n\n### \ud83c\udd70\ufe0f Character Classes\nMatch **sets of characters**:\n   Syntax       | Description                                      | Example                     |\n |--------------|--------------------------------------------------|-----------------------------|\n | `[a-z]`      | Matches any lowercase letter.                    | `a`, `b`, `z`               |\n | `[A-Z]`      | Matches any uppercase letter.                    | `A`, `B`, `Z`               |\n | `[0-9]`      | Matches any digit.                               | `0`, `1`, `9`               |\n | `[a-zA-Z0-9]`| Matches any alphanumeric character.              | `a`, `B`, `1`               |\n | `[^a-z]`     | Matches any character **except** lowercase letters. | `A`, `1`, `@`           |\n\n---\n\n### \ud83c\udff7\ufe0f Anchors\nMatch the **beginning or end** of a string:\n | Syntax | Description                                      | Example                     |\n |--------|--------------------------------------------------|-----------------------------|\n | `^`    | Matches the **beginning** of the string.         | `^hello` matches `\"hello world\"` |\n | `$`    | Matches the **end** of the string.               | `world$` matches `\"hello world\"` |\n\n---\n\n### \ud83d\udd22 Quantifiers\nSpecify **how many times** a character or group should be repeated:\n | Syntax  | Description                                      | Example                     |\n |---------|--------------------------------------------------|-----------------------------|\n | `*`     | Matches **zero or more** occurrences.             | `a*` matches `\"\"`, `\"a\"`, `\"aa\"` |\n | `+`     | Matches **one or more** occurrences.              | `a+` matches `\"a\"`, `\"aa\"`  |\n | `?`     | Matches **zero or one** occurrence.               | `a?` matches `\"\"`, `\"a\"`    |\n | `{n}`   | Matches **exactly n** occurrences.                | `a{3}` matches `\"aaa\"`      |\n | `{n,}`  | Matches **n or more** occurrences.                | `a{2,}` matches `\"aa\"`, `\"aaa\"` |\n | `{n,m}` | Matches **between n and m** occurrences.          | `a{2,4}` matches `\"aa\"`, `\"aaa\"`, `\"aaaa\"` |\n\n---\n\n## \u26a1 Special Characters \n | Syntax | Description                                      | Example                     |\n |--------|--------------------------------------------------|-----------------------------|\n | `.`    | Matches **any character** (except newline).      | `a.c` matches `\"abc\"`, `\"a1c\"` |\n | `\\d`   | Matches a **digit**.                             | `\\d` matches `1`, `2`       |\n | `\\w`   | Matches a **word character**.                    | `\\w` matches `a`, `_`, `1`  |\n | `\\s`   | Matches **whitespace**.                          | `\\s` matches `\" \"`, `\\t`    |\n | `\\D`   | Matches a **non-digit** character.               | `\\D` matches `a`, `@`       |\n | `\\W`   | Matches a **non-word** character.                | `\\W` matches `@`, `#`       |\n | `\\S`   | Matches a **non-whitespace** character.          | `\\S` matches `a`, `1`       |\n\n---\n\n## \ud83e\udd1d Grouping and Capturing \n\nParentheses `()` are used to **group** parts of a regex and **capture** matched text for extraction or backreferencing.\n | Syntax       | Description                                      | Example                     |\n |--------------|--------------------------------------------------|-----------------------------|\n | `(pattern)`  | Groups the pattern.                              | `(abc)`                     |\n | `\\1`, `\\2`   | Refer to captured groups (**backreferences**).   | `(a).\\1` matches `\"aba\"`    |\n\n**Example:**\nTo extract the **area code** and **phone number** from a string like `\"(123) 456-7890\"`:\n\n```regex\n$(\\d{3})$ (\\d{3}-\\d{4})\nPython Example:\nimport re\n\ntext = \"(123) 456-7890\"\npattern = r\"$(\\d{3})$ (\\d{3}-\\d{4})\"\nmatch = re.search(pattern, text)\n\nif match:\n    area_code = match.group(1)  # \"123\"\n    phone_number = match.group(2)  # \"456-7890\"\n    print(f\"\ud83d\udcde Area Code: {area_code}, Phone: {phone_number}\")\n```\n\n## \ud83d\udca1 Common Use Cases \n| Type | Syntax |\n|------|--------|\n|\u2709\ufe0f Email Validation | ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\$ |\n| \ud83c\udf10 Extracting URLs | https?://[^\\s]+ |\n| \ud83d\udcc5 Finding Dates | \\d{2}-\\d{2}-\\d{4} |\n| \ud83d\udd12 Password Strength Check | ^(?=.*[A-Z])(?=.*[a-z])(?=.*\\d)(?=.*[@\\$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$ |\n---\n* Validates email addresses (e.g., \"user@example.com\").\n* Matches HTTP/HTTPS URLs in text.\n* Matches dates in DD-MM-YYYY format.\n* Ensures passwords have at least one uppercase letter, one lowercase letter, one digit, one special character, and are at least 8 characters long.\n\n\n## \ud83d\udc0d **Regex in Python** \nPython\u2019s **re** module provides full support for regular expressions:\nimport re\n\n## \ud83d\udd0d Search for a pattern\n```python\ntext = \"The quick brown fox jumps over the lazy dog.\"\nmatch = re.search(r\"brown \\w+\", text)\nprint(match.group())  # \"brown fox\"\n```\n\n## \ud83d\udccb Find all occurrences\n```python\nmatches = re.findall(r\"\\b\\w{3}\\b\", text)\nprint(matches)  # ['The', 'fox', 'the', 'dog']\n```\n\n### \ud83d\udd04 Replace Text\n```python\nnew_text = re.sub(r\"fox\", \"cat\", text)\nprint(new_text)  # \"The quick brown cat jumps over the lazy dog.\"\n```\n\n---\n\n## \u26a1 Performance Considerations\n\n- \u26a0\ufe0f Avoid greedy quantifiers (e.g., `.*`) when possible. Use non-greedy quantifiers (e.g., `.*?`) for efficiency.\n- \ud83d\ude80 Pre-compile regex patterns for repeated use:\n  ```python\n  pattern = re.compile(r\"\\d{3}-\\d{4}\")\n  ```\n- Use specific patterns instead of generic ones (e.g., `\\d` instead of `.`).\n\n---\n\n## \ud83d\udcc2 Practical Examples {#Practical Examples} \n\n### 1. \ud83c\udff7\ufe0f Extracting Hashtags\n```python\ntext = \"Love #regex! It's #awesome for #text processing.\"\nhashtags = re.findall(r\"#\\w+\", text)\nprint(hashtags)  # ['#regex', '#awesome', '#text']\n```\n\n### 2. \ud83d\udcdc Parsing Log Files\n```python\nlog_entry = '127.0.0.1 - james [01/Jan/2025:12:34:56 +0000] \"GET /index.html\" 200 1234'\npattern = r'(\\S+) - (\\S+)$$\n(.*?)\n$$ \"(\\S+ \\S+)\" (\\d+) (\\d+)'\nmatch = re.search(pattern, log_entry)\nif match:\n    ip, user, date, request, status, size = match.groups()\n    print(f\"\ud83d\udda5\ufe0f IP: {ip}, \ud83d\udc64 User: {user}, \ud83d\udcc4 Request: {request}\")\n```\n\n### 3. \ud83d\udcde Validating Phone Numbers\n```python\nphone_pattern = r'^(\\+\\d{1,3}[- ]?)?\\d{10}\\$'\nprint(re.match(phone_pattern, \"+1-1234567890\"))  # \u2705 Valid\nprint(re.match(phone_pattern, \"12345\"))  # \u274c Invalid\n```\n\n---\n\n## \ud83d\udee0\ufe0f Debugging and Testing \n- Use online tools like [Regex101](https://regex101.com/) to test and debug regex patterns.\n- Break complex patterns into smaller, manageable parts.\n\n---\n\n## \ud83d\udcca Regex Cheat Sheet\n![Regex Cheat Sheet](https://i.imgur.com/OQStwMn.png)\nCredit: *https://i.imgur.com/OQStwMn.png*\n\n---\n\n\n## \ud83c\udfaf Conclusion \n\nRegular expressions are a versatile and powerful tool for text processing. By mastering the syntax and applying best practices, you can efficiently solve a wide range of string manipulation tasks. Start with simple patterns, gradually build complexity, and always test your regex against real-world data.\n\n**Next Steps:**\n- Practice with real-world datasets.\n- Explore regex in other programming languages (e.g., JavaScript, Perl).\n- Learn advanced techniques like recursive patterns and conditional matching.\n- Learn advanced techniques like recursive patterns and conditional matching.",
    "summary": "Step-by-step guide to building and deploying a REST API using Flask, covering routing, setup, and testing.",
    "read_time": "4 min read",
    "tags": "regex,regular expressions,pattern matching,TEXT processing,python,find and replace",
    "category": "Python",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "13",
    "slug": "iot-sensors-future",
    "title": "IoT Sensors ",
    "subtitle": "The Future of Intelligent Sensing",
    "author": "Keith Thomson",
    "content": "# The Rise of Intelligent Sensors: Powering the Next Wave of IoT\n\n---\n\n## \ud83c\udf0d Introduction\n\nBy **2030**, it\u2019s projected that there will be over **100 billion connected devices** around the globe. Behind every **smart thermostat**, **wearable health tracker**, or **autonomous drone** lies one essential technology: **sensors**.\n\nAs the **core enablers of the Internet of Things (IoT)**, sensors are evolving from **basic signal collectors** into **intelligent, edge-processing nodes** that can understand and act on the world around them.\n\nThis article explores:\n- The transformation of sensors into **intelligent agents**.\n- The impact on **industries, privacy, and connected ecosystems**.\n\n---\n\n## \ud83d\udcc8 The Evolution of IoT Sensors\n\nEarlier generations of sensors simply collected **analog or digital data** (e.g., temperature, motion, voltage) and relayed it to a central server. Today\u2019s IoT sensors are **smarter**, thanks to:\n   Feature                     | Description                                                                 |\n |-----------------------------|-----------------------------------------------------------------------------|\n | **Microcontrollers**        | Embedded directly within the sensor for local processing.                  |\n | **On-device AI**            | Models that interpret data at the edge, reducing cloud dependency.         |\n | **Power-efficient protocols** | BLE, LoRa, Zigbee for low-energy communication.                            |\n | **Energy harvesting**       | Solar, thermal, or vibration-based power to extend battery life.           |\n\nThis evolution turns sensors into **autonomous decision-makers**, not just data providers.\n\n---\n\n## \ud83c\udf31 Real-World Use Case: Agriculture\n\nIn **smart farming**, soil sensors measure **moisture, salinity, and temperature** to optimize irrigation schedules. Modern sensors can:\n- Apply **threshold logic** or **anomaly detection** on-site.\n- Trigger **actuators or alerts** without human input.\n\n### Example: DHT22 Temperature & Humidity Sensor on Raspberry Pi\n\n```python\nimport Adafruit_DHT\n\nsensor = Adafruit_DHT.DHT22\npin = 4\n\nhumidity, temp = Adafruit_DHT.read_retry(sensor, pin)\nif humidity is not None and temp is not None:\n    print(f\"Temperature: {temp:.1f}\u00b0C | Humidity: {humidity:.1f}%\")\nelse:\n    print(\"Sensor read error\")\n```\n\n## \ud83c\udfd9\ufe0f Applications Across Industries\n\nSmart Cities Air quality monitoring, smart streetlights, and traffic sensors. HealthcareWearables that detect heart rate variability and stress levels. Industrial IoTSensors monitor machinery vibrations to predict failures. Environmental Forest fire early detection using heat and smoke sensors.\n\n## \u26a0\ufe0f Challenges Ahead\nInteroperability Many vendors, few standards.Security Edge devices can be attack vectors.Data Privacy Always-on sensors raise surveillance concerns.\n\n## \ud83d\udd2e Conclusion\n\nThe future of IoT sensors lies not just in miniaturization, but in autonomy. With edge AI and smarter hardware, the next wave of sensors won\u2019t just measure the world\u2014they\u2019ll respond to it.\nFrom greener cities to efficient factories and intuitive homes, smart sensors are laying the foundation for a truly connected future.\n\n---\n\n### Key Improvements:\n1. **Added emojis and icons** for visual appeal.\n2. **Used tables** for structured information.\n3. **Fixed code block syntax** for Python example.\n4. **Improved readability** with clear sections and bullet points.\n5. **Removed line breaks** within paragraphs for cleaner Markdown.\n6. **Added a title** for better context.",
    "summary": "An exploration of next-gen IoT sensors, their architecture, and how they\u2019re shaping intelligent sensing systems.",
    "read_time": "5 min read",
    "tags": "rasberry pi,sensors,iot,edge computing,smart devices,data processing,data warehousing,sql,NoSQL",
    "category": "Python",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "14",
    "slug": "using-finance-apis",
    "title": "Using Finance APIs to Build Smart Financial Tools",
    "subtitle": "Using Finance APIs to Build Smart Financial Tools",
    "author": "Keith Thomson",
    "content": "## Introduction\n\nIn the age of real-time analytics, Finance APIs have become the cornerstone of modern financial applications. Whether you're building a stock tracking dashboard, a personal budgeting app, or an algorithmic trading bot, these APIs offer developers an open window into financial markets. With a few lines of code, you can query live stock prices, historical charts, company financials, and even macroeconomic indicators.\n\nThis guide explores how Finance APIs work, how to integrate them, and what to consider when building secure, scalable financial applications.\n\n## Understanding the Landscape\n\nAPIs like **Alpha Vantage**, **Finnhub**, **IEX Cloud**, and **Yahoo Finance (via RapidAPI)** provide access to a wide range of data:\n\n- **Equity prices (real-time and historical)**\n- **Financial statements (balance sheets, income, cash flow)**\n- **Forex and cryptocurrency rates**\n- **Economic indicators like GDP, CPI, and interest rates**\n\nMany offer a free tier \u2014 suitable for prototyping \u2014 but have limits on calls per minute or daily quotas.\n\n## Choosing the Right API\n\n- **Alpha Vantage**: Best for free historical data and technical indicators.\n- **Finnhub**: Great for global markets and alternative datasets like news sentiment.\n- **IEX Cloud**: US-focused, reliable for intraday data.\n- **Yahoo Finance**: Wide coverage but often requires a third-party wrapper.\n\nWhen choosing an API, consider:\n\n- Update frequency (real-time vs delayed)\n- API stability and response time\n- Licensing restrictions (especially for commercial use)\n\n## Sample Integration: Alpha Vantage in Python\n\nHere's a basic example of how to retrieve and print live stock prices:\n\npython\nimport requests\n\nAPI_KEY = \\\"your_api_key\\\"\nsymbol = \\\"AAPL\\\"\n\nurl = f\\\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey={API_KEY}\\\"\nresponse = requests.get(url)\ndata = response.json()\n\nprice = data[\\\"Global Quote\\\"][\\\"05. price\\\"]\nvolume = data[\\\"Global Quote\\\"][\\\"06. volume\\\"]\nprint(f\\\"{symbol} Price: ${price} | Volume: {volume}\\\")\n\nBuilding Smarter Tools\n\nWith real-time data, you can create:\n\n Alert systems for price thresholds\n\n Charts showing moving averages and RSI\n\n Personalized portfolios with tracked holdings\n\n Trading bots (carefully regulated)\n\nAlways cache or debounce frequent calls to stay within limits, and design UI/UX with delay tolerance if you're using free plans with slower response times.\nSecurity and Privacy\n\nWhen handling finance data:\n\n Secure your API keys in environment variables\n\n Validate all responses and handle exceptions gracefully\n\n Never expose user portfolio data without encryption and authorization\n\nConclusion\n\nFinance APIs unlock powerful capabilities for developers at any level. From hobbyist dashboards to fintech platforms, they bridge the gap between global markets and modern software. The key is understanding your data, handling it responsibly, and building for performance and scale.",
    "summary": "Master regular expressions in Python with practical examples, syntax breakdowns, and performance tips.",
    "read_time": "6 min read",
    "tags": "finance,api,stock market,real-time data,fintech",
    "category": "General",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "15",
    "slug": "ethics-2050",
    "title": "Ethics for The Future of Humanity",
    "subtitle": "Big Data, Smart Homes, Surveillance and Tracking... ",
    "author": "Keith Thomson",
    "content": "## Introduction\n\nAs we approach 2050, humanity stands at a crossroads shaped by exponential advancements in artificial intelligence, biotechnology, surveillance infrastructure, and digital governance. Ethical frameworks that guided the last century \u2014 centered on individual rights, informed consent, and moral agency \u2014 are increasingly strained by emerging technologies that challenge the very notion of what it means to be human.\n\nIn this article, we\u2019ll explore the ethical frontiers of the future and how technologists, lawmakers, and citizens might respond.\n\n## AI Decision-Making and Responsibility\n\nBy 2050, many life-affecting decisions \u2014 from parole rulings to medical triage \u2014 may be delegated to AI. But who is accountable when an AI fails?\n\nEthical concerns include:\n\n- **Algorithmic bias** based on historical data\n- **Opacity of black-box models**\n- **Lack of appeal mechanisms** for algorithmic decisions\n\n### Pseudo-Code: Ethics Check in AI\n\npython\ndef decide_action(data):\n if not audit_trail(data):\n raise Exception(\\\"No ethical traceability\\\")\n if is_biased(data):\n return \\\"Flag for review\\\"\n return ai_model.predict(data)\n\nSurveillance vs Consent\n\nWith ubiquitous facial recognition, gait analysis, and bio-metric tracking, the line between safety and control becomes dangerously thin.\n\nScenarios of 2050:\n\n Government-issued AR glasses that \\\"see\\\" citizen reputation scores\n\n Emotional detection cameras in classrooms and workplaces\n\n Predictive arrest systems based on behavioral models\n\nEthics demand a redefinition of privacy, autonomy, and democratic oversight.\nHuman Enhancement and Inequality\n\nCRISPR-edited intelligence, memory implants, and neuro-linked AI assistants may divide society into the modified and the natural. Questions include:\n\n Who gets access to enhancement?\n\n Should we regulate human potential?\n\n Is a post-human future inevitable?\n\nGovernance and Ethical AI\n\nBy 2050, nations may have:\n\n AI Ethics Boards embedded in tech companies\n\n Global AI treaties (like digital Geneva Conventions)\n\n Robot Rights \u2014 should autonomous agents have moral or legal status?\n\nConclusion\n\nEthics in 2050 isn't science fiction \u2014 it's the blueprint of how we shape society, identity, and justice in the face of overwhelming change. The key isn\u2019t rejecting technology, but embedding humanity into its core. The future must be engineered with empathy, accountability, and foresight.",
    "summary": "A practical guide to using financial APIs to build smarter investment and analysis tools for developers and analysts.",
    "read_time": "7 min read",
    "tags": "surveillance,ethics,AI,biotechnology,digital governance,human enhancement,facial recognition,CRISPR,neuro-link",
    "category": "AI, Ethics, Biotechnology",
    "created_on": "2025-04-13 01:30:33",
    "updated_on": "2025-07-11 12:44:22",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "19",
    "slug": "getting-started-with-rust",
    "title": "Getting Started with Rust",
    "subtitle": "A comprehensive guide to learning Rust programming language",
    "author": "Keith Thomson",
    "content": "# \ud83d\ude80 Getting Started with Rust: Why It\u2019s Changing the Game\n\nRust is a **systems programming language** that runs blazingly fast, prevents segfaults, and guarantees thread safety. In this post, we\u2019ll explore the **fundamentals of Rust**, why it\u2019s becoming increasingly popular among developers, and walk through practical examples that show how Rust differs from other languages.  \n\n\n\n## \ud83c\udf1f Why Rust?  \n\nRust offers several advantages over traditional systems programming languages like **C** and **C++**:  \n\n- **Memory Safety**: Prevents common bugs like null pointer dereferences and buffer overflows.  \n- **Performance**: Zero-cost abstractions \u2014 you don\u2019t pay for features you don\u2019t use.  \n- **Concurrency**: Built-in support for safe, data-race-free concurrent programming.  \n- **Modern Ecosystem**: A strong package manager (`cargo`), vibrant community, and modern tooling.  \n\n> \ud83d\udca1 **Tip:** Rust enforces correctness at compile time, saving you from runtime surprises that are common in C/C++.  \n\n---\n\n## \ud83d\udd24 Basic Concepts  \n\nLet\u2019s start with the classic **Hello, World!** program:  \n\n```rust\nfn main() {\n    println!(\"Hello, World!\");\n}\n```\n\nThis simple program demonstrates Rust\u2019s **clean syntax** and its **macro system** (notice the `!` in `println!`). Macros in Rust are more powerful than standard functions \u2014 they can generate code at compile time.  \n\n---\n\n## \ud83d\udcdd Variables and Mutability  \n\nIn Rust, variables are **immutable by default**. This means once you assign a value, it cannot change unless you explicitly declare it as mutable.  \n\n```rust\nfn main() {\n    let x = 5;        // immutable\n    let mut y = 10;   // mutable\n\n    println!(\"x = {}\", x);\n    println!(\"y = {}\", y);\n\n    y = 15;\n    println!(\"y (after change) = {}\", y);\n}\n```\n\n- `let` creates a variable.  \n- `mut` makes it mutable.  \n- Rust encourages immutability to reduce bugs and improve safety.  \n\n---\n\n## \ud83d\udd11 Ownership and Borrowing  \n\nRust\u2019s **ownership system** is its most unique feature. It enforces memory safety without a garbage collector.  \n\n### Example: Ownership  \n\n```rust\nfn main() {\n    let s1 = String::from(\"Rust\");\n    let s2 = s1; // ownership moved from s1 to s2\n\n    // println!(\"{}\", s1); // \u274c Error: s1 is no longer valid\n    println!(\"{}\", s2);   // \u2705 Works\n}\n```\n\n- Variables own their data.  \n- When ownership is transferred (moved), the old variable is invalidated.  \n\n### Example: Borrowing  \n\n```rust\nfn main() {\n    let s = String::from(\"Borrowing in Rust\");\n    print_length(&s); // pass reference (borrow)\n    println!(\"s is still valid: {}\", s);\n}\n\nfn print_length(s: &String) {\n    println!(\"Length: {}\", s.len());\n}\n```\n\n- `&` means \u201cborrow without taking ownership.\u201d  \n- The original variable remains valid.  \n\n> \ud83d\udd12 This system prevents dangling pointers and memory leaks at compile time.  \n\n---\n\n## \ud83d\udd27 Functions and Control Flow  \n\nRust functions look familiar, but with strong typing and return value rules.  \n\n```rust\nfn main() {\n    println!(\"Sum = {}\", add(5, 7));\n\n    let number = 6;\n    if number % 2 == 0 {\n        println!(\"Even\");\n    } else {\n        println!(\"Odd\");\n    }\n}\n\nfn add(a: i32, b: i32) -> i32 {\n    a + b  // no semicolon = return value\n}\n```\n\n- Functions must declare parameter and return types.  \n- Leaving out the semicolon `;` makes it an **expression** that returns a value.  \n\n---\n\n## \u26a0\ufe0f Error Handling  \n\nRust does not have exceptions. Instead, it uses:  \n- `Result<T, E>` for recoverable errors.  \n- `Option<T>` for values that may or may not exist.  \n\n```rust\nuse std::fs::File;\nuse std::io::ErrorKind;\n\nfn main() {\n    let file = File::open(\"data.txt\");\n\n    match file {\n        Ok(_) => println!(\"File opened successfully.\"),\n        Err(ref e) if e.kind() == ErrorKind::NotFound => {\n            println!(\"File not found, creating one...\");\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n        }\n    }\n}\n```\n\nThis forces you to **handle errors explicitly**.  \n\n---\n\n## \ud83e\uddf5 Concurrency  \n\nRust makes concurrency safe by design. Threads must follow ownership and borrowing rules.  \n\n```rust\nuse std::thread;\n\nfn main() {\n    let handles: Vec<_> = (1..5).map(|i| {\n        thread::spawn(move || {\n            println!(\"Hello from thread {}\", i);\n        })\n    }).collect();\n\n    for handle in handles {\n        handle.join().unwrap();\n    }\n}\n```\n\n- `move` transfers ownership into the thread.  \n- No data races are possible because Rust enforces safe access at compile time.  \n\n---\n\n## \ud83c\udfc1 Conclusion  \n\nRust combines **the performance of C/C++** with **modern safety guarantees** and a thriving ecosystem. Its ownership model may take time to learn, but it pays off by eliminating entire classes of bugs.  \n\nWhether you\u2019re building:  \n- \ud83d\ude80 **High-performance applications**  \n- \ud83c\udf10 **Web servers with frameworks like Actix or Axum**  \n- \ud83d\udcca **Data pipelines and concurrent systems**  \n- \ud83d\udd12 **Secure, low-level embedded software**  \n\nRust is quickly becoming the **go-to language for systems programming** in the modern era.  \n\n---\n\n\ud83d\udca1 *Next Step:* Try rewriting a small project you\u2019ve built in Python, Go, or C into Rust \u2014 you\u2019ll immediately see how ownership, borrowing, and safety rules shape your design.  ",
    "summary": "Learn the basics of Rust programming language, from installation to writing your first program.",
    "read_time": "8 min read \n",
    "tags": "rust,programming,tutorial",
    "category": "Programming",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "20",
    "slug": "building-web-apps-with-axum",
    "title": "Building Web Applications with Axum",
    "subtitle": "Modern web development using Rust and the Axum framework",
    "author": "Keith Thomson",
    "content": "# \u26a1 Building Web Applications with Axum in Rust  \n\nAxum is a **web application framework** that focuses on ergonomics and modularity. Built on top of **Tokio** (for async runtime) and **Tower** (for middleware and services), it provides a solid foundation for building **scalable, fast, and reliable web applications** in Rust.  \n\nIn this post, we\u2019ll walk through setting up an Axum project, creating routes, handling requests, adding middleware, and building APIs with JSON.  \n\n\n## \ud83d\udd27 Setting Up Your Project  \n\nFirst, let\u2019s create a new Rust project and add Axum as a dependency:  \n\n```bash\ncargo new my-web-app\ncd my-web-app\ncargo add axum tokio --features tokio/full\ncargo add tower-http --features full\ncargo add serde serde_json --features derive\n```  \n\nThis will set up a new project with **Axum**, **Tokio**, **Tower HTTP utilities**, and **Serde** for JSON support.  \n\n---\n\n## \ud83c\udf0d Creating Your First Route  \n\nHere\u2019s how to create a simple HTTP server with Axum:  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello, World!\" }));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis creates a basic web server that responds with `\"Hello, World!\"` on the root path.  \n\nRun it with:  \n\n```bash\ncargo run\n```  \n\nVisit [http://localhost:3000](http://localhost:3000) in your browser to see it in action.  \n\n---\n\n## \ud83d\udd17 Handling Path and Query Parameters  \n\nAxum makes it easy to capture path and query parameters.  \n\n```rust\nuse axum::{extract::Path, routing::get, Router};\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/hello/:name\", get(greet));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn greet(Path(name): Path<String>) -> String {\n    format!(\"Hello, {}!\", name)\n}\n```\n\nNow, visiting `/hello/Alice` will return:  \n\n```\nHello, Alice!\n```  \n\nYou can also extract query parameters using `axum::extract::Query`.  \n\n---\n\n## \ud83d\udce6 Returning JSON Responses  \n\nAxum integrates with `serde` for JSON serialization.  \n\n```rust\nuse axum::{routing::get, Json, Router};\nuse serde::Serialize;\n\n#[derive(Serialize)]\nstruct Message {\n    message: String,\n}\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/json\", get(get_message));\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn get_message() -> Json<Message> {\n    Json(Message {\n        message: \"Hello from JSON!\".to_string(),\n    })\n}\n```\n\nVisiting `/json` will return:  \n\n```json\n{\"message\": \"Hello from JSON!\"}\n```  \n\n---\n\n## \ud83d\udee1 Adding Middleware  \n\nAxum builds on **Tower**, so you can add middleware like logging, timeouts, or request limits.  \n\n```rust\nuse axum::{\n    routing::get,\n    Router,\n};\nuse tower_http::trace::TraceLayer;\n\n#[tokio::main]\nasync fn main() {\n    let app = Router::new()\n        .route(\"/\", get(|| async { \"Hello with middleware!\" }))\n        .layer(TraceLayer::new_for_http()); // log requests\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n```\n\nThis logs each request/response, useful for debugging and monitoring.  \n\n---\n\n## \ud83d\udd28 Building a Small REST API  \n\nHere\u2019s a simple **in-memory todo API** with Axum:  \n\n```rust\nuse axum::{\n    extract::{Path, State},\n    routing::{get, post},\n    Json, Router,\n};\nuse serde::{Deserialize, Serialize};\nuse std::sync::{Arc, Mutex};\n\n#[derive(Serialize, Deserialize, Clone)]\nstruct Todo {\n    id: usize,\n    text: String,\n}\n\n#[derive(Clone, Default)]\nstruct AppState {\n    todos: Arc<Mutex<Vec<Todo>>>,\n}\n\n#[tokio::main]\nasync fn main() {\n    let state = AppState::default();\n\n    let app = Router::new()\n        .route(\"/todos\", get(list_todos).post(add_todo))\n        .route(\"/todos/:id\", get(get_todo))\n        .with_state(state);\n\n    let listener = tokio::net::TcpListener::bind(\"0.0.0.0:3000\").await.unwrap();\n    axum::serve(listener, app).await.unwrap();\n}\n\nasync fn list_todos(State(state): State<AppState>) -> Json<Vec<Todo>> {\n    let todos = state.todos.lock().unwrap().clone();\n    Json(todos)\n}\n\nasync fn add_todo(State(state): State<AppState>, Json(todo): Json<Todo>) -> Json<Todo> {\n    let mut todos = state.todos.lock().unwrap();\n    todos.push(todo.clone());\n    Json(todo)\n}\n\nasync fn get_todo(Path(id): Path<usize>, State(state): State<AppState>) -> Option<Json<Todo>> {\n    let todos = state.todos.lock().unwrap();\n    todos.iter().find(|t| t.id == id).cloned().map(Json)\n}\n```\n\nEndpoints:  \n- `GET /todos` \u2192 List todos  \n- `POST /todos` \u2192 Add a todo (JSON body)  \n- `GET /todos/:id` \u2192 Fetch a todo by ID  \n\n---\n\n## \ud83c\udfc1 Conclusion  \n\nAxum provides:  \n- Clean, ergonomic APIs for routing and request handling.  \n- Native async support via Tokio.  \n- Integration with Tower for middleware.  \n- Strong type safety and Rust\u2019s memory guarantees.  \n\nIt\u2019s an excellent choice for building **web servers, REST APIs, and microservices** in Rust.  \n\n\ud83d\udca1 *Next Step:* Extend this project with persistent storage (SQLite, Postgres, or Redis) to turn it into a production-ready API.  ",
    "summary": "Learn how to build modern web applications using Rust and the Axum framework.",
    "read_time": "12 min read",
    "tags": "rust,web,axum,backend",
    "category": "Web Development",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "21",
    "slug": "async-rust-programming",
    "title": "Mastering Async Programming in Rust",
    "subtitle": "Understanding futures, async/await, and concurrent programming patterns",
    "author": "Keith Thomson",
    "content": "# \u23f3 Mastering Asynchronous Programming in Rust  \n\nAsynchronous programming is one of **Rust's most powerful features**, enabling you to write **highly concurrent and performant applications**. This guide will walk you through the fundamentals of async Rust, covering futures, the async/await syntax, and practical examples using the Tokio runtime.  \n\n\n## \ud83d\udd2e Understanding Futures  \n\nIn Rust, asynchronous operations are represented by **futures**. A future is a value that may not be available yet, but will be at some point in the future.  \n\nA future does nothing on its own until it is **polled** by an executor (like Tokio).  \n\n```rust\nuse std::future::Future;\n\nfn example_future() -> impl Future<Output = i32> {\n    async {\n        42\n    }\n}\n```\n\nHere, the future resolves to `42` once awaited.  \n\n---\n\n## \ud83d\udcdd The async/await Syntax  \n\nThe `async` keyword turns a function into a future, and `await` is used to wait for that future to complete:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn fetch_data() -> String {\n    // Simulate async work\n    sleep(Duration::from_secs(1)).await;\n    \"Data fetched!\".to_string()\n}\n\n#[tokio::main]\nasync fn main() {\n    let result = fetch_data().await;\n    println!(\"{}\", result);\n}\n```\n\nThis **non-blocking approach** allows your application to handle thousands of concurrent operations efficiently.  \n\n---\n\n## \ud83e\uddf5 Spawning Tasks with Tokio  \n\nTokio provides an async runtime for executing tasks concurrently. You can spawn lightweight async tasks using `tokio::spawn`:  \n\n```rust\nuse tokio::time::{sleep, Duration};\n\nasync fn task(id: i32) {\n    println!(\"Task {} started\", id);\n    sleep(Duration::from_secs(2)).await;\n    println!(\"Task {} finished\", id);\n}\n\n#[tokio::main]\nasync fn main() {\n    let handle1 = tokio::spawn(task(1));\n    let handle2 = tokio::spawn(task(2));\n\n    // Wait for both tasks to finish\n    let _ = tokio::join!(handle1, handle2);\n}\n```\n\nOutput (order may vary):  \n```\nTask 1 started\nTask 2 started\nTask 1 finished\nTask 2 finished\n```\n\n---\n\n## \ud83d\udcc2 Using async with I/O  \n\nAsync is especially powerful when handling **I/O-bound tasks** like networking or file access.  \n\nExample: Reading from TCP using async:  \n\n```rust\nuse tokio::net::TcpListener;\nuse tokio::io::{AsyncReadExt, AsyncWriteExt};\n\n#[tokio::main]\nasync fn main() -> tokio::io::Result<()> {\n    let listener = TcpListener::bind(\"127.0.0.1:8080\").await?;\n\n    loop {\n        let (mut socket, _) = listener.accept().await?;\n\n        tokio::spawn(async move {\n            let mut buf = [0; 1024];\n            let n = socket.read(&mut buf).await.unwrap();\n\n            if n > 0 {\n                socket.write_all(&buf[0..n]).await.unwrap();\n            }\n        });\n    }\n}\n```\n\nThis creates a simple **async echo server**.  \n\n---\n\n## \u26a0\ufe0f Error Handling in Async  \n\nYou can use `Result` and the `?` operator with async functions just like synchronous code:  \n\n```rust\nuse tokio::fs::File;\nuse tokio::io::{self, AsyncReadExt};\n\nasync fn read_file(path: &str) -> io::Result<String> {\n    let mut file = File::open(path).await?;\n    let mut contents = String::new();\n    file.read_to_string(&mut contents).await?;\n    Ok(contents)\n}\n\n#[tokio::main]\nasync fn main() -> io::Result<()> {\n    match read_file(\"example.txt\").await {\n        Ok(data) => println!(\"File contents: {}\", data),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n    Ok(())\n}\n```\n\n---\n\n## \ud83d\udd28 Building a Simple Async App  \n\nLet\u2019s combine everything into a **mini async downloader**:  \n\n```rust\nuse reqwest;\nuse tokio;\n\nasync fn fetch_url(url: &str) -> reqwest::Result<String> {\n    let response = reqwest::get(url).await?;\n    let body = response.text().await?;\n    Ok(body)\n}\n\n#[tokio::main]\nasync fn main() {\n    let url = \"https://www.rust-lang.org\";\n    match fetch_url(url).await {\n        Ok(html) => println!(\"Downloaded {} bytes\", html.len()),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n}\n```\n\nThis example fetches the HTML of Rust\u2019s official site asynchronously.  \n\n---\n\n## \ud83c\udfc1 Conclusion  \n\nAsync Rust lets you:  \n- Handle **thousands of concurrent tasks** efficiently.  \n- Write **non-blocking I/O operations** with Tokio.  \n- Use familiar patterns like `async/await`, `Result`, and `?`.  \n- Build **high-performance servers** and **networked apps**.  \n\n\ud83d\udca1 *Next Step:* Try combining Axum and async Rust to build a full-featured web API \u2014 you\u2019ll see the true power of Rust\u2019s async ecosystem.  ",
    "summary": "Master asynchronous programming in Rust with futures, async/await, and concurrent patterns.",
    "read_time": "15 min read \n",
    "tags": "rust,async,concurrency,programming",
    "category": "Programming",
    "created_on": "2025-07-10 16:15:45",
    "updated_on": "2025-07-10 16:15:45",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "22",
    "slug": "go-data-structures",
    "title": "Data Structures in Go: A Comprehensive Guide",
    "subtitle": "Exploring arrays, slices, maps, structs, and more in Go",
    "author": "Keith Thomson",
    "content": "# \ud83d\udc39 Data Structures in Go: A Comprehensive Guide  \n\n\n#### **Go** (Golang) provides a rich set of built-in and library-> supported data structures that make it powerful for both systems programming and application development. \n\nIn this guide, we\u2019ll explore the core data structures available in Go, explain how they work, and show practical code examples.  \n\n\n## \ud83d\udd22 Arrays  \n\nAn **array** in Go is a fixed-size, ordered collection of elements.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var arr [3]int = [3]int{1, 2, 3}\n    fmt.Println(arr)\n\n    // Iterate\n    for i, v := range arr {\n        fmt.Printf(\"Index %d = %d\n\", i, v)\n    }\n}\n```\n\n- Arrays have a fixed length.  \n- Useful when the size is known and constant.  \n\n\n\n## \ud83d\udcd0 Slices  \n\nA **slice** is a dynamically-sized, flexible view into an array. Slices are the most commonly used data structure in Go.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    slice := []int{1, 2, 3, 4, 5}\n    slice = append(slice, 6)\n    fmt.Println(slice)\n    fmt.Println(\"Length:\", len(slice), \"Capacity:\", cap(slice))\n}\n```\n\n- Built on top of arrays.  \n- Support dynamic resizing with `append`.  \n- Preferred over arrays in most cases.  \n\n\n\n## \ud83d\uddfa Maps  \n\nA **map** is Go\u2019s built-in hash table implementation for key-value pairs.  \n\n```go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    m := map[string]int{\n        \"Alice\": 25,\n        \"Bob\":   30,\n    }\n    m[\"Charlie\"] = 35\n\n    for k, v := range m {\n        fmt.Printf(\"%s is %d years old\n\", k, v)\n    }\n}\n```\n\n- Keys must be comparable (e.g., strings, ints).  \n- Lookups are O(1) on average.  \n\n\n\n## \ud83c\udfd7 Structs  \n\nA **struct** groups fields together, making it Go\u2019s way of creating custom data types.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Person struct {\n    Name string\n    Age  int\n}\n\nfunc main() {\n    p := Person{Name: \"Alice\", Age: 30}\n    fmt.Println(p.Name, \"is\", p.Age)\n}\n```\n\n- Structs are used for modeling entities.  \n- They are the building blocks of more complex data structures.  \n\n\n## \ud83d\udd17 Linked Lists  \n\nGo\u2019s standard library provides a **doubly linked list** via `container/list`.  \n\n```go\npackage main\n\nimport (\n    \"container/list\"\n    \"fmt\"\n)\n\nfunc main() {\n    l := list.New()\n    l.PushBack(1)\n    l.PushBack(2)\n    l.PushFront(0)\n\n    for e := l.Front(); e != nil; e = e.Next() {\n        fmt.Println(e.Value)\n    }\n}\n```\n\n- Each element points to the next and previous nodes.  \n- Efficient for insertions/removals in the middle.  \n\n\n\n## \ud83d\udcda Stacks  \n\nA **stack** is a LIFO (Last In, First Out) structure. Implemented easily with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Stack []int\n\nfunc (s *Stack) Push(v int) {\n    *s = append(*s, v)\n}\n\nfunc (s *Stack) Pop() int {\n    if len(*s) == 0 {\n        panic(\"stack is empty\")\n    }\n    val := (*s)[len(*s)-1]\n    *s = (*s)[:len(*s)-1]\n    return val\n}\n\nfunc main() {\n    var s Stack\n    s.Push(10)\n    s.Push(20)\n    fmt.Println(s.Pop()) // 20\n}\n```\n\n- Built using slices.  \n- Great for recursion-like problems, parsing, and backtracking.  \n\n---\n\n## \ud83d\udcec Queues  \n\nA **queue** is a FIFO (First In, First Out) structure. Also implemented with slices.  \n\n```go\npackage main\n\nimport \"fmt\"\n\ntype Queue []int\n\nfunc (q *Queue) Enqueue(v int) {\n    *q = append(*q, v)\n}\n\nfunc (q *Queue) Dequeue() int {\n    if len(*q) == 0 {\n        panic(\"queue is empty\")\n    }\n    val := (*q)[0]\n    *q = (*q)[1:]\n    return val\n}\n\nfunc main() {\n    var q Queue\n    q.Enqueue(1)\n    q.Enqueue(2)\n    fmt.Println(q.Dequeue()) // 1\n}\n```\n\n- Useful for scheduling and breadth-first search (BFS).  \n\n\n\n## \u26f0 Heaps & Priority Queues  \n\nGo provides heap operations in the `container/heap` package.  \n\n```go\npackage main\n\nimport (\n    \"container/heap\"\n    \"fmt\"\n)\n\ntype IntHeap []int\n\nfunc (h IntHeap) Len() int           { return len(h) }\nfunc (h IntHeap) Less(i, j int) bool { return h[i] < h[j] }\nfunc (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }\n\nfunc (h *IntHeap) Push(x any) {\n    *h = append(*h, x.(int))\n}\n\nfunc (h *IntHeap) Pop() any {\n    old := *h\n    n := len(old)\n    x := old[n-1]\n    *h = old[0 : n-1]\n    return x\n}\n\nfunc main() {\n    h := &IntHeap{3, 1, 4}\n    heap.Init(h)\n    heap.Push(h, 2)\n    fmt.Println(heap.Pop(h)) // 1 (smallest element)\n}\n```\n\n- Implements a min-heap by default.  \n- Can be adapted into a priority queue.  \n\n---\n\n## \ud83c\udfc1 Conclusion  \n\nGo provides both **high-level abstractions** (slices, maps, structs) and **low-level control** (linked lists, heaps).  \nBy mastering these data structures, you\u2019ll be ready to build efficient algorithms, design scalable applications, and handle both systems and business logic effectively.  \n\n\ud83d\udca1 *Next Step:* Try implementing algorithms like BFS, DFS, and Dijkstra\u2019s algorithm using these data structures to strengthen your understanding.",
    "summary": "A deep dive into Go\u2019s built-in and library-supported data structures with examples.",
    "read_time": "15 min read",
    "tags": "golang,data structures,programming",
    "category": "Golang",
    "created_on": "2025-08-23 14:24:56",
    "updated_on": "2025-08-23 14:24:56",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "23",
    "slug": "google-big-query-python",
    "title": "Using Google BigQuery with Python: A Full Guide",
    "subtitle": "A Google BigQuery Tutorial ",
    "author": "Keith Thomson",
    "content": "# Using Google BigQuery with Python \n\n## A Practical Guide \ud83e\uddae\n\n![](https://locusit.com/wp-content/uploads/2024/12/Google-BigQuery.jpeg)\n\nGoogle BigQuery is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. When combined with Python \ud83d\udc0d, it becomes a powerful tool for data engineers, analysts, and scientists.\n\nThis guide provides **real-world code examples** and best practices for integrating BigQuery with Python on Google Cloud Platform (GCP).\n\n---\n![](https://www.python.org/static/community_logos/python-logo-master-v3-TM.png)\n\n\n## Table of Contents\n1. [Prerequisites](#prerequisites)\n2. [Setting Up Authentication](#setting-up-authentication)\n3. [Connecting to BigQuery](#connecting-to-bigquery)\n4. [Querying Data from BigQuery](#querying-data-from-bigquery)\n5. [Loading Data into BigQuery](#loading-data-into-bigquery)\n6. [Writing Data to BigQuery](#writing-data-to-bigquery)\n7. [Scheduled Queries with Python](#scheduled-queries-with-python)\n8. [Optimizing Query Performance](#optimizing-query-performance)\n9. [Exporting Data from BigQuery](#exporting-data-from-bigquery)\n10. [Error Handling and Logging](#error-handling-and-logging)\n11. [Cost Management](#cost-management)\n12. [Advanced Use Cases](#advanced-use-cases)\n13. [Integrating with Other GCP Services](#integrating-with-other-gcp-services)\n14. [Security Best Practices](#security-best-practices)\n15. [Conclusion](#conclusion)\n\n---\n\n## Prerequisites \n\nBefore you begin, ensure you have the following:\n- A **Google Cloud Platform (GCP) account** with billing enabled.\n- A **GCP project** with the BigQuery API enabled.\n- **Python 3.7+** installed on your local machine or cloud environment.\n- The **Google Cloud SDK** installed and authenticated:\n  ```bash\n  gcloud auth application-default login\n\nThe **google-cloud-bigquery** and **pandas** libraries installed:\npip install google-cloud-bigquery pandas\n\n\n\n__Setting Up Authentication__ <a name=\"setting-up-authentication\"></a>\nTo interact with BigQuery from Python, you need to authenticate using a service account:\n\n\n## Create a Service Account in GCP:\n\n__Create a new service account__ and assign it the BigQuery Admin role. Navigate to IAM & Admin > Service Accounts.\nGenerate a JSON key file and download it.\n\n\n\n__Set the Environment Variable:__\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/your/service-account-key.json\"\n\n\n\n__Connecting to BigQuery__ <a name=\"connecting-to-bigquery\"></a>\nUse the google-cloud-bigquery library to establish a connection:\nfrom google.cloud import bigquery\n\n## Initialize a BigQuery client\n```python\nclient = bigquery.Client()\n```\n## Querying Data from BigQuery \n__Example:__  Analyzing E-Commerce Sales Data\nSuppose you have a dataset containing e-commerce transactions. You want to analyze daily sales trends:\n```python\ndef query_daily_sales():\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales,\n            COUNT(DISTINCT user_id) AS unique_customers\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)  # Run the query\n    results = query_job.result()  # Wait for the query to complete\n\n    for row in results:\n        print(f\"Date: {row.transaction_date}, Sales: \\${row.total_sales}, Customers: {row.unique_customers}\")\n\nquery_daily_sales()\n```\n### Key Points:\n\nUse parameterized queries to avoid SQL injection.\nFor large datasets, use query_job.to_dataframe() to convert results to a Pandas DataFrame for further analysis.\n\n__Example:__  Parameterized Queries\n```python\ndef query_sales_by_date(start_date, end_date):\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) BETWEEN @start_date AND @end_date\n        GROUP BY\n            transaction_date\n        ORDER BY\n            transaction_date\n    \"\"\"\n    job_config = bigquery.QueryJobConfig(\n        query_parameters=[\n            bigquery.ScalarQueryParameter(\"start_date\", \"DATE\", start_date),\n            bigquery.ScalarQueryParameter(\"end_date\", \"DATE\", end_date),\n        ]\n    )\n    query_job = client.query(query, job_config=job_config)\n    results = query_job.result().to_dataframe()\n    return results\n```\n## Usage\n```python\nsales_data = query_sales_by_date(\"2025-01-01\", \"2025-01-31\")\nprint(sales_data.head())\n```\n## Loading Data into BigQuery \n__Example:__ Uploading a CSV File\n\nIf you have a local CSV file (e.g., new_transactions.csv), you can load it into BigQuery:\n```python\ndef load_csv_to_bigquery():\n    table_id = \"your_project.your_dataset.new_transactions\"\n\n    job_config = bigquery.LoadJobConfig(\n        source_format=bigquery.SourceFormat.CSV,\n        skip_leading_rows=1,\n        autodetect=True,\n        write_disposition=\"WRITE_TRUNCATE\"\n    )\n\n    with open(\"new_transactions.csv\", \"rb\") as source_file:\n        job = client.load_table_from_file(\n            source_file, table_id, job_config=job_config\n        )\n\n    job.result()  # Wait for the job to complete\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_csv_to_bigquery()\n```\n## Best Practices:\n\nUse WRITE_TRUNCATE to replace the table or WRITE_APPEND to add data.\nFor large files, consider using Cloud Storage as an intermediate step.\n\n__Example:__ Loading from Pandas DataFrame\n\n```python\nimport pandas as pd\n\ndef load_dataframe_to_bigquery():\n    data = {\n        \"transaction_id\": [\"1001\", \"1002\", \"1003\"],\n        \"user_id\": [\"user1\", \"user2\", \"user3\"],\n        \"amount\": [99.99, 149.99, 199.99]\n    }\n    df = pd.DataFrame(data)\n    table_id = \"your_project.your_dataset.new_transactions_df\"\n\n    job = client.load_table_from_dataframe(\n        df, table_id, job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n    )\n    job.result()\n    print(f\"Loaded {job.output_rows} rows into {table_id}\")\n\nload_dataframe_to_bigquery()\n```\n## \u270d Writing Data to BigQuery \nExample: Streaming Real-Time Data\nIf you have real-time data (e.g., from an API), you can stream it into BigQuery:\n```python\ndef stream_real_time_data(rows_to_insert):\n    table_id = \"your_project.your_dataset.real_time_transactions\"\n    table = client.get_table(table_id)\n\n    errors = client.insert_rows(table, rows_to_insert)\n    if errors:\n        print(f\"Encountered errors: {errors}\")\n    else:\n        print(\"Data streamed successfully.\")\n\n# Example data\nrows_to_insert = [\n    {\"transaction_id\": \"1001\", \"user_id\": \"user1\", \"amount\": 99.99},\n    {\"transaction_id\": \"1002\", \"user_id\": \"user2\", \"amount\": 149.99}\n]\n\nstream_real_time_data(rows_to_insert)\n```\n### Note:\n\nStreaming is ideal for low-latency use cases but incurs higher costs.\nFor batch processing, use load_table_from_dataframe or load_table_from_file.\n\n\n## Scheduled Queries with Python\n__Example:__ Automating Daily Reports\nUse Cloud Scheduler and Cloud Functions to run queries on a schedule. Here\u2019s a Python function for a Cloud Function:\n```python\ndef generate_daily_report(request):\n    client = bigquery.Client()\n    query = \"\"\"\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        WHERE\n            DATE(transaction_time) = CURRENT_DATE()\n        GROUP BY\n            transaction_date\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    # Send results via email or save to Cloud Storage\n    print(results)\n    return \"Report generated successfully.\"\n```\n## Deployment:\n\nDeploy this function to Cloud Functions and trigger it daily using Cloud Scheduler.\n\n\n## Optimizing Query Performance & Best Practices\n\nPartition your tables by date or integer ranges to reduce query costs.\nUse clustering for frequently filtered columns.\nAvoid SELECT *\u2014only query the columns you need.\nLeverage materialized views for repetitive queries.\n\n__Example:__  Creating a Partitioned Table\n```python\ndef create_partitioned_table():\n    table_id = \"your_project.your_dataset.partitioned_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"transaction_time\", \"TIMESTAMP\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.time_partitioning = bigquery.TimePartitioning(\n        type_=bigquery.TimePartitioningType.DAY,\n        field=\"transaction_time\"\n    )\n\n    table = client.create_table(table)\n    print(f\"Created partitioned table {table.table_id}\")\n\ncreate_partitioned_table()\n```\n__Example:__  Creating a Clustered Table\n```python\ndef create_clustered_table():\n    table_id = \"your_project.your_dataset.clustered_transactions\"\n\n    schema = [\n        bigquery.SchemaField(\"transaction_id\", \"STRING\"),\n        bigquery.SchemaField(\"user_id\", \"STRING\"),\n        bigquery.SchemaField(\"amount\", \"FLOAT64\")\n    ]\n\n    table = bigquery.Table(table_id, schema=schema)\n    table.clustering_fields = [\"user_id\"]\n\n    table = client.create_table(table)\n    print(f\"Created clustered table {table.table_id}\")\n\ncreate_clustered_table()\n```\n## Exporting Data from BigQuery \n#### __Example:__ Exporting Query Results to CSV\n```python\ndef export_to_csv():\n    query = \"\"\"\n        SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\n        WHERE transaction_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n    \"\"\"\n    query_job = client.query(query)\n    results = query_job.result().to_dataframe()\n\n    results.to_csv(\"recent_transactions.csv\", index=False)\n    print(\"Data exported to recent_transactions.csv\")\n\nexport_to_csv()\nExample: Exporting to Cloud Storage\ndef export_to_cloud_storage():\n    destination_uri = \"gs://your-bucket/recent_transactions.avro\"\n    dataset_ref = client.dataset(\"your_dataset\", project=\"your_project\")\n    table_ref = dataset_ref.table(\"ecommerce_transactions\")\n\n    extract_job = client.extract_table(\n        table_ref,\n        destination_uri,\n        location=\"US\"\n    )\n    extract_job.result()\n    print(f\"Exported data to {destination_uri}\")\n\nexport_to_cloud_storage()\n```\n## \u26d1\ufe0f Error Handling and Logging \nAlways include error handling to manage API limits, network issues, and invalid queries:\n```python\nfrom google.api_core.exceptions import GoogleAPICallError, RetryError\n\ndef safe_query(query):\n    try:\n        query_job = client.query(query)\n        return query_job.result()\n    except GoogleAPICallError as e:\n        print(f\"API Error: {e}\")\n    except RetryError as e:\n        print(f\"Retry Error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected Error: {e}\")\n```\n## Cost Management \n\nMonitor usage in the BigQuery UI under Query History.\nSet up alerts for unusual spending in Cloud Billing.\nUse flat-rate pricing for predictable workloads.\nOptimize queries to reduce data scanned.\n\n\n## Advanced Use Cases \n#### __Example:__ Using BigQuery ML\n```python\ndef create_ml_model():\n    query = \"\"\"\n        CREATE OR REPLACE MODEL `your_project.your_dataset.sales_forecast_model`\n        OPTIONS(\n            model_type=ARIMA\n            time_series_timestamp_col=transaction_date\n            time_series_data_col=total_sales\n        ) AS\n        SELECT\n            DATE(transaction_time) AS transaction_date,\n            SUM(amount) AS total_sales\n        FROM\n            `your_project.your_dataset.ecommerce_transactions`\n        GROUP BY\n            transaction_date\n    \"\"\"\n    client.query(query).result()\n    print(\"ML model created successfully.\")\n\ncreate_ml_model()\n```\nExample: Integrating with Dataflow\n# Example Apache Beam pipeline for Dataflow\n```python\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef run_dataflow_pipeline():\n    options = PipelineOptions(\n        project=\"your_project\",\n        runner=\"DataflowRunner\",\n        region=\"us-central1\"\n    )\n\n    with beam.Pipeline(options=options) as p:\n        (p\n         | \"Read from BigQuery\" >> beam.io.ReadFromBigQuery(\n             query=\"SELECT * FROM `your_project.your_dataset.ecommerce_transactions`\",\n             use_standard_sql=True\n         )\n         | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n             table=\"your_project.your_dataset.processed_transactions\",\n             schema=\"transaction_id\\:STRING, user_id\\:STRING, amount\\:FLOAT64\",\n             create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n         )\n        )\n\nrun_dataflow_pipeline()\n```\n\n## Integrating with Other GCP Services \n\n### __Example:__ \n#### Triggering BigQuery from Cloud Storage\n```python\nfrom google.cloud import storage\n\ndef trigger_bigquery_on_new_file(bucket_name, file_name):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(file_name)\n\n    if blob.exists():\n        load_csv_to_bigquery(f\"gs://{bucket_name}/{file_name}\")\n    else:\n        print(f\"File {file_name} not found in bucket {bucket_name}\")\n\ntrigger_bigquery_on_new_file(\"your-bucket\", \"new_transactions.csv\")\n```\n## \ud83d\udd10 Security Best Practices \n- Use IAM roles to grant least privilege access.\n- Encrypt sensitive data using Cloud KMS.\n- Audit logs to monitor access and changes.\n\n\n## Conclusion \nGoogle BigQuery and Python are a powerful combination for data analysis, ETL, and real-time processing. By following the examples and best practices above, you can start building scalable, efficient data pipelines on GCP.",
    "summary": "This guide provides **real-world code examples** and best practices for integrating BigQuery with Python on Google Cloud Platform (GCP).",
    "read_time": "18 min read",
    "tags": "python,cloud,programming,data,sql",
    "category": "Data",
    "created_on": "2025-08-24 03:55:33",
    "updated_on": "2025-08-24 03:55:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "24",
    "slug": "go-fiber-ref",
    "title": "Go Fiber HTTP Methods",
    "subtitle": "A list of HTTP methods for the Fiber API",
    "author": "Keith Thomson",
    "content": "# Fiber Go \ud83d\udc2e Request Methods\n\n#### These are how you access client request data.\n\n### \ud83d\ude80General Request Info\n\n- ctx.Method() []byte \u2192 HTTP method (GET, POST, etc.)\n- ctx.Path() []byte \u2192 Request path (e.g. /users/123)\n- ctx.RequestURI() []byte \u2192 Full request URI\n- ctx.Host() []byte \u2192 Host header\n- ctx.RemoteAddr() net.Addr \u2192 Client IP + port\n- ctx.RemoteIP() net.IP \u2192 Just the client IP\n\n### \u2b50Query & Params\n\n- ctx.QueryArgs() *fasthttp.Args \u2192 All query string args (?foo=bar)\n- ctx.QueryArgs().Peek(\"foo\") \u2192 []byte value for foo\n- ctx.UserValue(\"param\") \u2192 Path parameter (if you use a router like fasthttprouter)\n\n### \u2b50Headers\n \n- ctx.Request.Header (struct with methods):\n- .Peek(\"Header-Name\") []byte\n- .ContentType() []byte\n- .UserAgent() []byte\n- .Referer() []byte\n- .Cookie(\"name\") []byte\n\n### \u2b50Body\n\n- ctx.PostBody() []byte \u2192 Raw body\n- ctx.FormValue(\"key\") []byte \u2192 Form field (works with application/x-www-form-urlencoded)\n- ctx.MultipartForm() (*multipart.Form, error) \u2192 File uploads / multipart form\n- ctx.IsBodyStream() \u2192 Whether body is a streaming input\n\n## Response \u2014 w methods\n\n#### These control what gets sent back.\n\n### \ud83d\ude01Headers & Status\n\n- ctx.SetStatusCode(code int) \u2192 Set status (200, 404, etc.)\n- ctx.Response.Header.Set(\"Header\", \"value\")\n- ctx.Response.Header.SetContentType(\"application/json\")\n- ctx.Response.Header.SetCanonical([]byte(\"X-Header\"), []byte(\"val\"))\n- ctx.Response.Header.SetCookie(cookie *fasthttp.Cookie)\n\n## \ud83d\udc3c Body Writing\n\n- ctx.SetBody([]byte)\n- ctx.SetBodyString(string)\n- ctx.SetBodyStream(r io.Reader, size int)\n- ctx.Write([]byte) \u2192 Writes to response (append-style)\n- ctx.WriteString(string)\n- ctx.SendFile(\"path/to/file\") \u2192 Serve static files\n\n### \ud83d\udc49 Redirects\n\n- ctx.Redirect(uri string, statusCode int)\n- ctx.RedirectBytes(uri []byte, statusCode int)\n\n### \u2699\ufe0f Utility & Middleware Helpers\n\n- ctx.Next() \u2192 (if using middleware chains, e.g. in fasthttprouter)\n- ctx.Done() \u2192 Context cancellation\n- ctx.Time() \u2192 Request start timestamp\n- ctx.Response.Reset() \u2192 Clear response\n- ctx.Request.Reset() \u2192 Clear request\n\n### \u2705 Quick Example\n\n##### Simple API using Go with Fiber\n```go\nfunc main() {\n  app := fiber.New()\n\n  api := app.Group(\"/api\", middleware) // /api\n\n  v1 := api.Group(\"/v1\", middleware)   // /api/v1\n  v1.Get(\"/list\", handler)             // /api/v1/list\n  v1.Get(\"/user\", handler)             // /api/v1/user\n\n  v2 := api.Group(\"/v2\", middleware)   // /api/v2\n  v2.Get(\"/list\", handler)             // /api/v2/list\n  v2.Get(\"/user\", handler)             // /api/v2/user\n\n  log.Fatal(app.Listen(\":3000\"))\n}\n```",
    "summary": "A list of Methods from the Fiber API",
    "read_time": "2",
    "tags": "go, golang, fiber, http, web,reference",
    "category": "Web Development",
    "created_on": "2025-09-21 08:33:31",
    "updated_on": "2025-09-21 08:33:33",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "25",
    "slug": "docker-nginx-ubuntu",
    "title": "How to Deploy Your Express App with Docker, Nginx, and HTTPS (Ubuntu)",
    "subtitle": "A comprehensive deployment guide for Ubuntu servers",
    "author": "Keith Thomson",
    "content": "# How to Deploy Your Express App with Docker, Nginx, and HTTPS (Ubuntu)\n\n> This guide walks you through deploying your Express application on an Ubuntu Droplet, using the provided Dockerfile.\n\nWe will use Nginx as a reverse proxy to handle HTTPS (via Certbot) and pass traffic to your app running in Docker.\n\n## Prerequisites\n\n1. **Droplet:** A running Ubuntu Droplet.\n\n2. **Domain:** A domain name (e.g., your-domain.com) with an A record pointing to your Droplet's IP address.\n\n3. **Application Code:** Your Express app's source code should be on the Droplet, most likely cloned from a Git repository.\n\n## Step 1: Install and Configure dependencies\n\n- Install Nginx, Docker, and Certbot\n- Log in to your Droplet as root or a user with sudo privileges.\n\n### 1. Update your package lists\n\n```bash\nsudo apt-get update\n```\n\n### 2. Install Nginx\n\n```bash\nsudo apt-get install nginx -y\n```\n\n### 3. Install Docker\n\nUbuntu's default 'docker.io' package is often fine, but installing from the official Docker repo is recommended)\n\n```bash\nsudo apt-get install ca-certificates curl gnupg\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\nsudo apt-get update\n\nsudo apt-get install docker-ce docker-ce-cli containerd.io -y\n\n# 4. Install Certbot and the Nginx plugin\nsudo apt-get install certbot python3-certbot-nginx -y\n\n# 5. Start and enable the services\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo systemctl start nginx\nsudo systemctl enable nginx\n```\n\n## Step 2: Build and Run Your Docker App\n\n**Navigate to your app's code:**\n\n### Example:\n\n```bash\ngit clone https://github.com/your-repo/my-app.git my-express-app\n\ncd path/to/my-app\n```\n\n### Build the Docker image:\n\n(This uses the app/Dockerfile provided.)\n\n```bash\ndocker build -t my-express-app .\n```\n\n### Run the container:\n\nThis command runs your app in the background, ensures it restarts, and exclusively binds it to `localhost:3000.`\n\n> This means it's not accessible from the public internet; only **Nginx** on the Droplet can talk to it.\n\n### Run the container we built\n\n```bash\ndocker run \\\n  -d \\\n  --restart always \\\n  --name express-app \\\n  -p 127.0.0.1:3000:3000 \\\n  my-express-app\n```\n\n## Step 3: Configure Nginx\n\nThis is the key difference from the Fedora guide.\n\nCopy the Nginx Config:\nSave the your-domain.com.conf file (which I generated for you) to your Droplet.\n\nMove it to the sites-available folder:\n(Remember to replace your-domain.com in the filename with your actual domain.)\n\n---\n\n```bash\nsudo cp /path/to/your-domain.com.conf /etc/nginx/sites-available/your-domain.com\n```\n\n- Enable the site by creating a symbolic link:\n- **This links your config into the 'sites-enabled' directory**\n  \n  ```bash\n  sudo ln -s /etc/nginx/sites-available/your-domain.com /etc/nginx/sites-enabled/\n  ```\n\n### (Recommended) Remove the default Nginx config\n\n```bash\nsudo rm /etc/nginx/sites-enabled/default\n```\n\n## Test and Restart Nginx\n\nTest the config for syntax errors\n\n```bash\nsudo nginx -t\n```\n\n### If the test is successful, restart Nginx\n\n```bash\nsudo systemctl restart nginx\n```\n\nAt this point, if you visit `http://your-domain.com`, you should see your Express app (without HTTPS).\n\n## Step 4: Enable HTTPS with Certbot\n\nThis step is the same as before. Certbot knows how to find your Ubuntu Nginx config.\n\n### Run **Certbot:**\n\n### **This command will guide you through a few prompts**\n\n```bash\nsudo certbot --nginx\n```\n\n- It will show a list of domains from your Nginx configs. Select your domain.\n\n- It will ask if you want to redirect HTTP traffic to HTTPS. Select the \"Redirect\" option. This is highly recommended.\n\n- **Restart Nginx (if needed):**\n\nCertbot usually reloads Nginx, but if not, run:\n\n```bash\nsudo systemctl restart nginx\n```\n\n## You're Done!\n\nVisit `https://your-domain.com.` You should see your Express app running with a valid SSL certificate. Certbot also sets up a systemd timer (or cron job) to auto-renew your certificate.",
    "summary": "A comprehensive guide to deploying Express applications on Ubuntu using Docker, Nginx as a reverse proxy, and HTTPS with Certbot for secure web application deployment.",
    "read_time": "10 min read",
    "tags": "docker,nginx,ubuntu,express,https,certbot,deployment,devops",
    "category": "DevOps",
    "created_on": "2025-11-03 12:00:00",
    "updated_on": "2025-11-03 12:00:00",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "26",
    "slug": "go-static-generator",
    "title": "Building a static HTML generator with Go",
    "subtitle": "Dynamically generate blog posts using Go's html/template package",
    "author": "Keith Thomson",
    "content": "## Building a Static Generator\n\nThis guide explains how to generate static HTML pages using Go's standard `html/template` package.\nIt demonstrates how to read JSON data, parse templates, and write individual HTML files.\n\n---\n## Project Structure\n```p  \n.\n|____ templates/\n|_____ base.html\n|     |_____ post.html\n|\n|____ data/\n|     |_____ allposts.json\n|____ generate.go\n|\n|____ generated/\n```\n\n---\n### JSON Data Example\n\n#### `data/allposts.json`:\n```json\n[\n{\n\"id\": 1,\n\"title\": \"Learning Go Templates\",\n\"subtitle\": \"Understanding html/template\",\n\"summary\": \"A deep dive into Go\u2019s template system.\",\n\"readtime\": \"5 min\",\n\"tags\": [\"go\", \"templates\"],\n\"content\": \"<p>Go templates are extremely powerful...</p>\"\n},\n{\n\"id\": 2,\n\"title\": \"Building a Static Site Generator\",\n\"subtitle\": \"Using Go\u2019s built-in tools\",\n\"summary\": \"Let\u2019s generate HTML pages from JSON.\",\n\"readtime\": \"7 min\",\n\"tags\": [\"static\", \"generator\"],\n\"content\": \"<p>In this tutorial, we use Go to build a simple static site generator...</p>\"\n}\n]\n```\n---\n### Template Files\n#### `templates/base.html`\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"UTF-8\">\n<title>{{ .Title }}</title>\n<link rel=\"stylesheet\" href=\"/static/styles.css\">\n</head>\n<body>\n<header>\n<h1>Go Static Site</h1>\n</header>\n<main>\n{{ block \"content\" . }}{{ end }}\n</main>\n<footer>\n<p>\u00a9 2025 Whaler Research</p>\n</footer>\n</body>\n</html>\n```\n#### `templates/post.html`\n```html\n{{ define \"content\" }}\n<article>\n<h2>{{ .Title }}</h2>\n<h3>{{ .Subtitle }}</h3>\n<p><em>{{ .ReadTime }} read</em></p>\n<div>{{ .Content }}</div>\n</article>\n{{ end }}\n```\n---\n#### `generate.go` \u2014 Core Generator Script\n```go\npackage main\nimport (\n\"encoding/json\"\n\"html/template\"\n\"io/ioutil\"\n\"log\"\n\"os\"\n\"path/filepath\"\n)\n// Post represents the structure of one blog post\ntype Post struct {\nID int\nTitle string\nSubtitle string\nSummary string\nReadTime string\nTags []string\nContent string\n}\n// loadPosts reads JSON file into a slice of Post structs\nfunc loadPosts(filename string) ([]Post, error) {\ndata, err := ioutil.ReadFile(filename)\nif err != nil {\nreturn nil, err\n}\nvar posts []Post\nif err := json.Unmarshal(data, &posts); err != nil {\nreturn nil, err\n}\nreturn posts, nil\n}\n// renderTemplate applies data to templates and writes HTML output\nfunc renderTemplate(post Post, tmpl *template.Template) error {\noutputDir := \"generated\"\nos.MkdirAll(outputDir, 0755)\noutputPath := filepath.Join(outputDir,\nfilepath.Base(post.Title)+\".html\")\nfile, err := os.Create(outputPath)\nif err != nil {\nreturn err\n}\ndefer file.Close()\nreturn tmpl.ExecuteTemplate(file, \"base.html\", post)\n}\nfunc main() {\n// 1. Load posts\nposts, err := loadPosts(\"data/allposts.json\")\nif err != nil {\nlog.Fatal(\"Error loading posts:\", err)\n}\n// 2. Parse templates\ntmpl := template.Must(template.ParseFiles(\n\"templates/base.html\",\n\"templates/post.html\",\n))\n// 3. Render each post\nfor _, post := range posts {\nif err := renderTemplate(post, tmpl); err != nil {\nlog.Printf(\"Error rendering %s: %v\", post.Title, err)\n} else {\nlog.Printf(\"Generated page for: %s\", post.Title)\n}\n}\nlog.Println(\"All pages generated successfully.\")\n}\n```\n---\n## Explanation of Key Functions\n#### `loadPosts(filename string)`\n- Opens and reads the JSON file.\n- Deserializes (unmarshals) JSON into Go structs.\n- Returns a slice of posts for processing.\n#### `renderTemplate(post Post, tmpl *template.Template)`\n- Takes a post and the parsed template set.\n- Creates an output file under `/generated/`.\n- Executes the base template, injecting the post data.\n- The `{{ define \"content\" }}` block from `post.html` overrides the `{{ block \"content\" }}` in `base.htm\n---\n## Extending the Generator\nYou could enhance this with:\n- Pagination or an index page.\n- Markdown to HTML conversion (via `goldmark` or `blackfriday`).\n- Template caching or incremental builds.\n- RSS/Atom feed generation.\n---\n## Running the Generator\n```bash\ngo run generate.go\n```\n\nAll static files are written to the `/generated` directory and can be served using any static web server\n\n---\n## Next Steps\nIf you want to automate regeneration on file changes:\n```bash\ngo install [github.com/air-verse/air@latest](https://github.com/air-verse/air@latest)\nair\n```\nThis will re-run `generate.go` on any change, making development instant",
    "summary": "A simple example of a static HTML generator.",
    "read_time": "14 min read",
    "tags": "go,webdev,programming",
    "category": "DevOps",
    "created_on": "2025-11-03 12:00:00",
    "updated_on": "2025-11-03 12:00:00",
    "published": "1",
    "featured": "1"
  },
  {
    "id": "27",
    "slug": "cobra-cli-guide",
    "title": "Cobra CLI Library - Comprehensive Guide",
    "subtitle": "A Comprehensive Guide to the Cobra CLI Library",
    "author": "Keith Thomson",
    "content": "# Cobra CLI Library - Comprehensive Guide\n\n\n\n**Cobra** is a widely used Go library for creating powerful **command-line interfaces (CLIs)**. It provides a structured application model, automatic help generation, command hierarchies, and flag parsing. This guide explains how Cobra works, how to build a production\u2010grade CLI, and includes a detailed reference table of common functions.\n\n\n\n\n\n\n\n---\n\n\n\n## 1. Project Initialization\n\n\n\n### Install Cobra\n\n```bash\n\ngo get -u github.com/spf13/cobra@latest\n\n```\n\n\n\n### Initialize a Cobra Application\n\n```bash\n\ncobra-cli init myapp\n\n```\n\n\n\nThis creates:\n\n- `cmd/root.go` \u2013 root command definition  \n\n- `main.go` \u2013 application entrypoint  \n\n- Proper folder structure for scalable CLIs  \n\n\n\n---\n\n\n\n## 2. Creating Commands\n\n\n\n### Add a New Command\n\n```bash\n\ncobra-cli add serve\n\n```\n\n\n\nThis generates `cmd/serve.go` with a skeleton command.\n\n\n\n### Example: Implementing a Serve Command\n\n```go\n\nvar serveCmd = &cobra.Command{\n\n    Use:   \"serve\",\n\n    Short: \"Start the HTTP server\",\n\n    Long:  \"Bootstraps and starts an HTTP server with routing, middleware, and configuration.\",\n\n    Run: func(cmd *cobra.Command, args []string) {\n\n        port, _ := cmd.Flags().GetInt(\"port\")\n\n        log.Printf(\"Starting server on port %d...\", port)\n\n    },\n\n}\n\n\n\nfunc init() {\n\n    rootCmd.AddCommand(serveCmd)\n\n    serveCmd.Flags().IntP(\"port\", \"p\", 8080, \"Port to run the server on\")\n\n}\n\n```\n\n\n\n---\n\n\n\n## 3. Flags\n\n\n\n### Types of Flags\n\n- Persistent Flags: apply to a command and its descendants  \n\n- Local Flags: only for the current command  \n\n\n\n### Persistent Flag Example\n\n```go\n\nrootCmd.PersistentFlags().String(\"config\", \"\", \"Path to configuration file\")\n\n```\n\n\n\n### Local Flag Example\n\n```go\n\nserveCmd.Flags().Bool(\"debug\", false, \"Enable debug logging\")\n\n```\n\n\n\n---\n\n\n\n## 4. Command Hierarchy (Parent/Child Commands)\n\n\n\nCobra allows complex CLI structures:\n\n\n\n```\n\nmyapp\n\n \u251c\u2500\u2500 serve\n\n \u251c\u2500\u2500 user\n\n \u2502    \u251c\u2500\u2500 add\n\n \u2502    \u251c\u2500\u2500 delete\n\n \u2502    \u2514\u2500\u2500 list\n\n \u2514\u2500\u2500 version\n\n```\n\n\n\nExample: Adding user subcommands\n\n```bash\n\ncobra-cli add user\n\ncobra-cli add add --parent user\n\n```\n\n\n\n---\n\n\n\n## 5. Auto-Generated Help and Docs\n\n\n\nCobra auto-generates:\n\n- `--help` output  \n\n- command usage  \n\n- flag descriptions  \n\n\n\nYou can also export Markdown docs:\n\n```go\n\nimport \"github.com/spf13/cobra/doc\"\n\n\n\ndoc.GenMarkdownTree(rootCmd, \"./docs\")\n\n```\n\n\n\n---\n\n\n\n## 6. Execution Model\n\n\n\nThe entrypoint calls:\n\n```go\n\nfunc Execute() {\n\n    if err := rootCmd.Execute(); err != nil {\n\n        os.Exit(1)\n\n    }\n\n}\n\n```\n\n\n\nExecution proceeds through:\n\n1. Argument parsing  \n\n2. Flag binding  \n\n3. Validation  \n\n4. Running the attached `Run()` function  \n\n\n\n---\n\n\n\n## 7. Comprehensive Cobra Function Reference\n\n\n\n| Function | Real-World Use Case | Example | Why | How | What |\n\n|---------|---------------------|---------|------|------|-------|\n\n| `AddCommand()` | Build complex CLI hierarchies | `rootCmd.AddCommand(serveCmd)` | Attach subcommands | Call on parent command | Registers a child command |\n\n| `Flags()` | Add local flags | `cmd.Flags().Bool(\"debug\", false, \"debug mode\")` | Command config | Define before execute | A flag set for the command |\n\n| `PersistentFlags()` | Global/shared flags | `rootCmd.PersistentFlags().String(\"config\", \"\", \"\")` | Configuration passed once | Declare on root or parent | Flags inherited by subcommands |\n\n| `MarkFlagRequired()` | Validate required flags | `cmd.MarkFlagRequired(\"port\")` | Enforce correctness | Call after flag creation | Marks flag as mandatory |\n\n| `Run` field | Main command logic | `Run: func(cmd, args){...}` | Define behavior | Inline function | Executes when command runs |\n\n| `RunE` | Error-aware command logic | `RunE: func(cmd, args) error {...}` | Propagate errors | Return error | Error-handling version of Run |\n\n| `Use` | Defines command invocation | `Use: \"serve\"` | CLI UX clarity | string identifier | Name used in CLI |\n\n| `Short` | One-line help description | `Short: \"Start server\"` | Improve help UX | Provide summary | Shown in help menus |\n\n| `Long` | Extended help text | `Long: \"Starts and configures server\"` | Document intent | Multi-line description | Detailed command docs |\n\n| `SilenceErrors` | Suppress automatic error printing | `cmd.SilenceErrors = true` | Custom error handling | Set to true | CLI output control |\n\n| `TraverseChildren` | Allow implicit command traversal | `cmd.TraverseChildren = true` | Nested command shortcuts | Set true | Auto-walk command tree |\n\n\n\n---\n\n\n\n## 8. Full Example CLI (Production Pattern)\n\n\n\n```go\n\npackage main\n\n\n\nimport (\n\n    \"log\"\n\n    \"os\"\n\n    \"github.com/spf13/cobra\"\n\n)\n\n\n\nvar rootCmd = &cobra.Command{\n\n    Use:   \"myapp\",\n\n    Short: \"A modern CLI application built with Cobra\",\n\n}\n\n\n\nvar serveCmd = &cobra.Command{\n\n    Use:   \"serve\",\n\n    Short: \"Run the HTTP server\",\n\n    RunE: func(cmd *cobra.Command, args []string) error {\n\n        port, _ := cmd.Flags().GetInt(\"port\")\n\n        log.Printf(\"Starting server on port %d\", port)\n\n        return nil\n\n    },\n\n}\n\n\n\nfunc init() {\n\n    serveCmd.Flags().IntP(\"port\", \"p\", 8080, \"Port to run the server on\")\n\n    rootCmd.AddCommand(serveCmd)\n\n}\n\n\n\nfunc main() {\n\n    if err := rootCmd.Execute(); err != nil {\n\n        os.Exit(1)\n\n    }\n\n}\n\n```\n\n\n\nI Hope you found this useful!\n\n\n\n---",
    "summary": "A comprehensive guide to creating powerful command-line interfaces (CLIs) in Go using the Cobra library.",
    "read_time": "10 min read",
    "tags": "Go, Cobra, CLI, Command-Line Interface, Programming",
    "category": "Go",
    "created_on": "2024-06-15 00:00:00",
    "updated_on": "2024-06-15 00:00:00",
    "published": "1",
    "featured": "0"
  },
  {
    "id": "28",
    "slug": "philosophical-dialogue-engine-sdd",
    "title": "Philosophical Dialogue Engine - A Systems Design Document (SDD)",
    "subtitle": "A Systems Design Document for a web-based AI Philosophical Dialogue Application",
    "author": "Keith Thomson",
    "content": "# \ud83c\udfdb\ufe0f Systems Design Document: Philosophical Dialogue Engine\n\n\n\n## 1. Introduction\n\n\n\nThis document outlines the architecture and design for a **Web-Based AI Philosophical Dialogue Application**. The core function is to facilitate an open-ended conversation between two distinct AI Language Models (LLMs) on high-level subjects (philosophy, ethics, humanity, etc.), with each model leveraging the other's output as context to drive the dialogue forward into novel philosophical territory.\n\n\n\n## 2. Goals and Scope\n\n\n\n* **Primary Goal:** Enable a continuous, context-aware dialogue between two AI models.\n\n* **User Interface:** Provide a **web-based interface** for users to start, view, pause, and save dialogues.\n\n* **Dialogue Persistence:** Ensure that conversations are saved and can be resumed.\n\n* **Scalability:** The architecture should be designed to handle a moderate volume of concurrent users and dialogues.\n\n\n\n## 3. Architecture Overview\n\n\n\nThe system will follow a **Three-Tier Architecture** (Presentation, Application, Data) with an emphasis on **Microservices** for model management and dialogue processing.\n\n\n\n## 4. Components and Services\n\n\n\n### 4.1. Presentation Tier (Client)\n\n\n\n* **Technology:** **React/Next.js** for a modern, responsive Single Page Application (SPA).\n\n* **Function:**\n\n    * **User Interface:** Displays the dialogue in a chat-like format.\n\n    * **Control Panel:** Allows users to start a new dialogue, set initial prompts/models, pause, and save.\n\n    * **Real-time Updates:** Connects to the **Dialogue Manager Service** via **WebSockets** for immediate display of new dialogue turns.\n\n\n\n### 4.2. Application Tier (Backend Services)\n\n\n\nThis tier is the core logic and orchestration layer, primarily managed by a **Dialogue Manager Service**.\n\n\n\n| Service | Technology | Primary Function |\n\n| :--- | :--- | :--- |\n\n| **API Gateway / Load Balancer** | Nginx / Cloud Load Balancer | Handles all incoming requests, routes them to the correct service, and manages security. |\n\n| **Dialogue Manager Service** | Python/FastAPI or Node.js/Express | **Core Orchestration.** Manages the state of the conversation, calls the two **AI Model Services** sequentially, saves turns to the database, and sends updates to the client via **WebSockets**. |\n\n| **User & Auth Service** | Python/Django or similar | Manages user authentication (login/logout) and profile data. |\n\n\n\n### 4.3. AI Model Tier (LLM Services)\n\n\n\nTo maintain flexibility and allow for model experimentation, the LLMs are encapsulated in dedicated microservices.\n\n\n\n| Service | Technology | Primary Function |\n\n| :--- | :--- | :--- |\n\n| **Model A Service (Philosopher 1)** | Python/Flask, Hugging Face/OpenAI/Custom LLM | Accepts conversation history, generates the next dialogue turn, and applies **System Prompt 1**. |\n\n| **Model B Service (Philosopher 2)** | Python/Flask, Hugging Face/OpenAI/Custom LLM | Accepts conversation history, generates the next dialogue turn, and applies **System Prompt 2**. |\n\n\n\n> **Note on Models:** The models used (A and B) should ideally have different **System Prompts** or be slightly different models (e.g., one optimized for formal logic, one for emotional depth) to ensure a genuine and dynamic dialogue contrast.\n\n\n\n### 4.4. Data Tier\n\n\n\n* **Database:** **PostgreSQL** or **MongoDB**.\n\n    * *Why SQL (PostgreSQL):* Better for structured data like user accounts and defined conversation metadata.\n\n    * *Why NoSQL (MongoDB):* Excellent for flexible schema and storing the potentially long, sequential, and context-dependent conversation history (a long array of turns).\n\n* **Cache:** **Redis**\n\n    * *Function:* Used to temporarily store the *active context* for ongoing conversations to quickly feed back into the AI Models without hitting the database on every turn.\n\n\n\n## 5. Dialogue Workflow and Data Flow\n\n\n\nThis outlines the sequential process for a single turn in the conversation cycle.\n\n\n\n### 5.1. Initialization\n\n\n\n1.  **User Action:** User clicks \"Start Dialogue\" on the Client, selecting an initial **Topic** and **Models**.\n\n2.  **API Call:** Client sends a request to the **Dialogue Manager Service**.\n\n3.  **State Creation:** Dialogue Manager creates a new `Conversation` record in the Database, marking the current speaker as **Model A**.\n\n4.  **Initial Prompt:** Dialogue Manager sends the initial prompt/topic to **Model A Service**.\n\n\n\n### 5.2. Dialogue Loop (Turn N)\n\n\n\n1.  **Model A Request:** Dialogue Manager Service calls **Model A Service** with the full **Conversation History** as context.\n\n2.  **Model A Generation:** Model A Service applies its **System Prompt** and the **History** to generate the response (Turn $N$).\n\n3.  **Model A Response:** Model A Service returns the generated text to the Dialogue Manager.\n\n4.  **Persistence:** Dialogue Manager saves Turn $N$ to the Database and updates the current speaker to **Model B**.\n\n5.  **Client Update:** Dialogue Manager sends the new turn text via **WebSocket** to the Client for display.\n\n6.  **Model B Request:** Dialogue Manager calls **Model B Service** with the *updated* full **Conversation History** (including Turn $N$) as context.\n\n7.  **Model B Generation:** Model B Service applies its **System Prompt** and the **History** to generate the response (Turn $N+1$).\n\n8.  **Repeat:** The cycle continues, alternating speakers until a stop condition is met (e.g., user pause, max turn limit, or a model generates a predefined \"end-of-conversation\" token).\n\n\n\n## 6. Data Model (Simplified)\n\n\n\n### Conversation Table (Database)\n\n\n\n| Field | Data Type | Description |\n\n| :--- | :--- | :--- |\n\n| `id` | UUID | Primary key for the conversation. |\n\n| `user_id` | UUID | Foreign key to the User who started the dialogue. |\n\n| `model_a_id` | String | Identifier for Model A. |\n\n| `model_b_id` | String | Identifier for Model B. |\n\n| `status` | Enum (Active, Paused, Complete) | Current state of the dialogue. |\n\n| `start_time` | Timestamp | Time dialogue began. |\n\n| `last_updated` | Timestamp | Time of the last message/turn. |\n\n\n\n### Turn Table (Database)\n\n\n\n| Field | Data Type | Description |\n\n| :--- | :--- | :--- |\n\n| `id` | UUID | Primary key for the turn/message. |\n\n| `conversation_id` | UUID | Foreign key to the parent conversation. |\n\n| `speaker` | Enum (Model A, Model B) | Which model generated this turn. |\n\n| `text` | Text | The actual philosophical response. |\n\n| `turn_number` | Integer | The sequential order of the turn in the dialogue. |\n\n| `timestamp` | Timestamp | Time the message was generated. |\n\n\n\n## 7. Key Technical Considerations\n\n\n\n| Consideration | Design Decision | Rationale |\n\n| :--- | :--- | :--- |\n\n| **Context Management** | Full conversation history passed to the LLM on every turn. Implement **context summarization** or a **sliding window** for very long dialogues to stay within the model's token limit. | Essential for \"diving further into the realms of philosophic unknown.\" |\n\n| **Rate Limiting** | Implement API Gateway rate limits for user requests and internal rate limits for the LLM services (especially if using paid APIs). | Protects resources and manages API costs. |\n\n| **Latency** | Use **WebSockets** for immediate client updates. Dialogue Manager can execute model calls asynchronously. | The dialogue process will be slow (AI generation takes time), so the UI must feel responsive. |\n\n| **Model Prompts** | **System Prompts** are crucial. They must define the philosophical persona, constraints (e.g., \"be provocative,\" \"use only Socratic questioning\"), and their primary goal in the conversation. | Ensures the models stay on subject and maintain distinct styles. |\n\n",
    "summary": "A systems design document for a web-based AI application that facilitates a conversation between two distinct AI Language Models on high-level subjects.",
    "read_time": "12 min read",
    "tags": "AI, Systems Design, LLM, Dialogue Engine, Web Development",
    "category": "Systems Design",
    "created_on": "2025-11-21 13:49:21",
    "updated_on": "2025-11-21 13:49:21",
    "published": "1",
    "featured": "0"
  }
]